% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chat-groq.R
\name{chat_groq_developer}
\alias{chat_groq_developer}
\title{Chat with Groq AI Models (Developer Version)}
\usage{
chat_groq_developer(
  system_prompt = NULL,
  base_url = "https://api.groq.com/openai/v1",
  api_key = NULL,
  credentials = NULL,
  model = NULL,
  params = NULL,
  api_args = list(),
  echo = NULL,
  api_headers = character()
)
}
\arguments{
\item{system_prompt}{A system prompt to set the behavior of the assistant.}

\item{base_url}{Base URL for Groq API.}

\item{api_key}{\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#deprecated}{\figure{lifecycle-deprecated.svg}{options: alt='[Deprecated]'}}}{\strong{[Deprecated]}} Use \code{credentials} instead.}

\item{credentials}{Override the default credentials. You generally should not
need this argument; instead set the \code{GROQ_API_KEY} environment variable.
The best place to set this is in \code{.Renviron}, which you can easily edit by
calling \code{usethis::edit_r_environ()}.

If you do need additional control, this argument takes a zero-argument
function that returns either a string (the API key), or a named list
(added as additional headers to every request).}

\item{model}{The model to use for the chat (defaults to "openai/gpt-oss-20b").
We regularly update the default, so we strongly recommend explicitly
specifying a model for anything other than casual use.
Use \code{\link[=models_groq]{models_groq()}} to see all options.}

\item{params}{Common model parameters, usually created by \code{\link[ellmer:params]{ellmer::params()}}.}

\item{api_args}{Additional arguments passed to the API.}

\item{echo}{Whether to echo the conversation to the console. One of:
\itemize{
\item \code{"none"}: No output
\item \code{"output"}: Echo assistant responses only
\item \code{"all"}: Echo all messages
Defaults to \code{"output"} in interactive sessions, \code{"none"} otherwise.
}}

\item{api_headers}{Additional HTTP headers.}
}
\value{
A \link[ellmer:Chat]{ellmer::Chat} object with methods for:
\itemize{
\item \verb{$chat()}: Send messages and receive responses
\item \verb{$chat_structured()}: Extract structured data with type validation
\item Batch processing via \code{\link[=batch_chat]{batch_chat()}} and \code{\link[=batch_chat_structured]{batch_chat_structured()}}
\item Parallel processing via \code{\link[=parallel_chat]{parallel_chat()}} and \code{\link[=parallel_chat_structured]{parallel_chat_structured()}}
}
}
\description{
Creates a chat interface for Groq's API with support for structured outputs,
batch processing, and parallel execution. This developer version extends
ellmer's ProviderOpenAICompatible to inherit full batch support while adding
Groq-specific schema formatting (additionalProperties: false).

Sign up at \url{https://groq.com}.

Built on top of ellmer's ProviderOpenAICompatible class.
}
\section{Structured Outputs}{

Groq supports strict JSON schema validation with guaranteed compliance when using
compatible models. See \url{https://console.groq.com/docs/structured-outputs} for details.
}

\section{Batch Processing}{

Groq's batch API offers 50\% cost discount and no rate limit impact. Batch jobs
are processed asynchronously with completion windows from 24 hours to 7 days.
Use \code{\link[=batch_chat]{batch_chat()}} or \code{\link[=batch_chat_structured]{batch_chat_structured()}} to submit batches.
}

\section{Models}{

For strict structured output support, use:
\itemize{
\item \code{"openai/gpt-oss-20b"} (default)
\item \code{"openai/gpt-oss-120b"}
}

Other models support best-effort JSON output but may not guarantee schema compliance.
}

\examples{
\dontrun{
# Basic chat
chat <- chat_groq_developer()
chat$chat("What is the capital of France?")

# Structured output
type_person <- ellmer::type_object(
  name = ellmer::type_string(),
  age = ellmer::type_integer(),
  city = ellmer::type_string()
)
result <- chat$chat_structured(
  "John is 30 years old and lives in NYC",
  type = type_person
)

# Batch processing
results <- batch_chat(
  chat,
  prompts = c("Question 1?", "Question 2?", "Question 3?")
)

# Parallel processing
results <- parallel_chat(
  chat,
  prompts = c("Question 1?", "Question 2?", "Question 3?")
)
}

}
\seealso{
\itemize{
\item \code{\link[=batch_chat]{batch_chat()}} for batch processing
\item \code{\link[=parallel_chat]{parallel_chat()}} for parallel processing
\item \code{\link[ellmer:type_boolean]{ellmer::type_object()}}, \code{\link[ellmer:type_boolean]{ellmer::type_array()}} for defining types
}
}
\concept{chatbots}
